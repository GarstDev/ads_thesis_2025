{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "r62qBqGdiW1e",
        "AWSvqgZfibRJ",
        "cYrBixAYifpE",
        "KBqZnbg4jSBp",
        "_NS4bf2mkcIq",
        "fsqQUeqDr_VA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading dataset"
      ],
      "metadata": {
        "id": "r62qBqGdiW1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import frequently used libraries\n",
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Install Dutch spacy model\n",
        "!python -m spacy download nl_core_news_sm\n",
        "\n",
        "# Install rouge_score\n",
        "!pip install rouge_score\n",
        "from rouge_score import rouge_scorer"
      ],
      "metadata": {
        "id": "mzMlOy59BK2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMzWt1xUsULf"
      },
      "outputs": [],
      "source": [
        "# Upload dataset\n",
        "df = pd.read_csv(\n",
        "    'INSERT DATASET',\n",
        "    encoding='utf-8',\n",
        "    header=0,\n",
        "    sep=';',\n",
        "    quotechar='\"',   # handles commas inside quotes\n",
        ")\n",
        "\n",
        "\n",
        "# Preview df\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for column types and completeness\n",
        "df.info()"
      ],
      "metadata": {
        "id": "vBLKSXeXvxYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete column 'Unnamed: 12' - transformation error\n",
        "df.drop(columns='Unnamed: 12', inplace=True)"
      ],
      "metadata": {
        "id": "Q5LXgDEPGfeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check df again after deletion of column\n",
        "df.info()"
      ],
      "metadata": {
        "id": "J-wjvEEnG56t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Transform data into string objects for subsequent analysis.\n",
        "'''"
      ],
      "metadata": {
        "id": "pHxs5zQoQQYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure correct column types for analysis\n",
        "df['human_label'] = df['human_label'].astype(str)\n",
        "df['cot_gpt_label'] = df['cot_gpt_label'].astype(str)\n",
        "df['mp_gpt_label'] = df['mp_gpt_label'].astype(str)\n",
        "df['ps_gpt_label'] = df['ps_gpt_label'].astype(str)\n",
        "df['sc_gpt_label'] = df['sc_gpt_label'].astype(str)"
      ],
      "metadata": {
        "id": "0wkjoiaFI_mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure correct column types for analysis\n",
        "df['human_explanation'] = df['human_explanation'].astype(str)\n",
        "df['cot_gpt_explanation'] = df['cot_gpt_explanation'].astype(str)\n",
        "df['mp_gpt_explanation'] = df['mp_gpt_explanation'].astype(str)\n",
        "df['ps_gpt_explanation'] = df['ps_gpt_explanation'].astype(str)\n",
        "df['sc_gpt_explanation'] = df['sc_gpt_explanation'].astype(str)"
      ],
      "metadata": {
        "id": "DyC408uuHL5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "OxwETEVrIq4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''The preliminary checks are complete and analysis can be started.\n",
        "We start with the Cohen's Kappa agreement between the human expert and\n",
        "the gpt evaluations.\n",
        "'''"
      ],
      "metadata": {
        "id": "0yHfPsH1QVeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cohen's Kappa human expert-gpt"
      ],
      "metadata": {
        "id": "AWSvqgZfibRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "COHEN'S KAPPA"
      ],
      "metadata": {
        "id": "nhLmlHCNKb9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This section calculates Cohen's Kappa score to measure the agreement between human expert classifications\n",
        "and the classifications generated by various GPT prompting strategies (CoT, MP, PS, SC).\n",
        "Scores are calculated overall, per specific criterion, and per individual case.\n",
        "'''\n",
        "\n",
        "# Unique values\n",
        "criteria = df['criteria'].unique()\n",
        "case_ids = df['case_id'].unique()\n",
        "gpt_variants = ['cot', 'mp', 'ps', 'sc']\n",
        "\n",
        "# Cohen's Kappa per prompt strategy\n",
        "print(\"\\n Cohenâ€™s Kappa per prompt strategy:\")\n",
        "for variant in gpt_variants:\n",
        "    kappa = cohen_kappa_score(df['human_label'], df[f'{variant}_gpt_label'])\n",
        "    print(f\"  {variant.upper():>3} vs Human: {kappa:.3f}\")\n",
        "\n",
        "# Cohen's Kappa Per Criterium\n",
        "print(\"\\n Cohen's Kappa per criterium:\")\n",
        "for variant in gpt_variants:\n",
        "    print(f\"\\n{variant.upper()} vs Human:\")\n",
        "    for crit in criteria:\n",
        "        subset = df[df['criteria'] == crit]\n",
        "        kappa = cohen_kappa_score(subset['human_label'], subset[f'{variant}_gpt_label'])\n",
        "        print(f\"  {crit:<15}: {kappa:.3f}\")\n",
        "\n",
        "# Cohen's Kappa Per Case\n",
        "print(\"\\n Cohen's Kappa per case:\")\n",
        "for variant in gpt_variants:\n",
        "    print(f\"\\n{variant.upper()} vs Human:\")\n",
        "    for case in case_ids:\n",
        "        subset = df[df['case_id'] == case]\n",
        "        kappa = cohen_kappa_score(subset['human_label'], subset[f'{variant}_gpt_label'])\n",
        "        print(f\"  Case {case}: {kappa:.3f}\")\n"
      ],
      "metadata": {
        "id": "shTlvnFeKdCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "For better interpretability, the scores will also be plotted using bar plots.\n",
        "'''"
      ],
      "metadata": {
        "id": "lYcVg4wiQDNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare summary df per criterium\n",
        "results = []\n",
        "for variant in gpt_variants:\n",
        "    for crit in criteria:\n",
        "        subset = df[df['criteria'] == crit]\n",
        "        kappa = cohen_kappa_score(subset['human_label'], subset[f'{variant}_gpt_label'])\n",
        "        results.append({'Prompt Strategy': variant.upper(), 'Criterion': crit, 'Cohen Kappa': kappa})\n",
        "\n",
        "summary_df = pd.DataFrame(results)\n",
        "\n",
        "# Plot Cohen's Kappa per criterium\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=summary_df, x='Criterion', y='Cohen Kappa', hue='Prompt Strategy')\n",
        "plt.title('Cohen\\'s Kappa per Criterion by Prompting Stategy')\n",
        "plt.ylim(0,1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qsy9jS5-MFp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare summary df per case\n",
        "results = []\n",
        "for variant in gpt_variants:\n",
        "    for case in case_ids:\n",
        "        subset = df[df['case_id'] == case]\n",
        "        kappa = cohen_kappa_score(subset['human_label'], subset[f'{variant}_gpt_label'])\n",
        "        results.append({'Prompt Strategy': variant.upper(), 'Case': case, 'Cohen Kappa': kappa})\n",
        "\n",
        "case_df = pd.DataFrame(results)\n",
        "\n",
        "# Plot Cohen's Kappa per case\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=case_df, x='Case', y='Cohen Kappa', hue='Prompt Strategy')\n",
        "plt.title('Cohen\\'s Kappa per Case by Prompt Strategy')\n",
        "plt.ylim(0,1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KrQITNLeMT6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To visualize the inter-relationships better,\n",
        "the scores are also represented as a heatmap.\n",
        "'''\n",
        "gpt_variants = ['cot', 'mp', 'ps', 'sc']\n",
        "criteria = df['criteria'].unique()\n",
        "case_ids = df['case_id'].unique()\n",
        "\n",
        "# Define the min and max values to ensure consistent scaling across heatmaps\n",
        "vmin, vmax = -0.4, 1\n",
        "\n",
        "# Heatmap per Criterion\n",
        "results_crit = []\n",
        "for crit in criteria:\n",
        "    for variant in gpt_variants:\n",
        "        subset = df[df['criteria'] == crit]\n",
        "        kappa = cohen_kappa_score(subset['human_label'], subset[f'{variant}_gpt_label'])\n",
        "        results_crit.append({'Criterion': crit, 'Prompt Strategy': variant.upper(), 'Kappa': kappa})\n",
        "\n",
        "heatmap_data_crit = pd.DataFrame(results_crit).pivot(index='Criterion', columns='Prompt Strategy', values='Kappa')\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(heatmap_data_crit, annot=True, cmap='coolwarm', vmin=vmin, vmax=vmax, fmt=\".3f\")\n",
        "plt.title(\"Cohen's Kappa per Criterion by Prompt Strategy\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Heatmap per Case\n",
        "results_case = []\n",
        "for case in case_ids:\n",
        "    for variant in gpt_variants:\n",
        "        subset = df[df['case_id'] == case]\n",
        "        kappa = cohen_kappa_score(subset['human_label'], subset[f'{variant}_gpt_label'])\n",
        "        results_case.append({'Case': case, 'Prompt Strategy': variant.upper(), 'Kappa': kappa})\n",
        "\n",
        "heatmap_data_case = pd.DataFrame(results_case).pivot(index='Case', columns='Prompt Strategy', values='Kappa')\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(heatmap_data_case, annot=True, cmap='coolwarm', vmin=vmin, vmax=vmax, fmt=\".3f\")\n",
        "plt.title(\"Cohen's Kappa per Case by Prompt Strategy\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xVN1yQIYNx5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROUGE EVALUATION"
      ],
      "metadata": {
        "id": "cYrBixAYifpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This section evaluates the quality of GPT-generated explanations by comparing\n",
        "them against human expert explanations using ROUGE scores.\n",
        "ROUGE-1, ROUGE-2, and ROUGE-L F1 scores are calculated to assess\n",
        "unigram, bigram, and longest sequence overlap.\n",
        "'''"
      ],
      "metadata": {
        "id": "eNNSbn81KqQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE"
      ],
      "metadata": {
        "id": "DgvszdfNOWst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dutch Spacy model for tokenization and text processing\n",
        "nlp = spacy.load(\"nl_core_news_sm\")\n",
        "\n",
        "def preprocess(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Normalize percentages: replace \"10%\" -> \"10 procent\"\n",
        "    text = re.sub(r'(\\d+)%', r'\\1 procent', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "\n",
        "    # Use spacy tokenizer\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Normalize white space\n",
        "    tokens = [token.text for token in doc if not token.is_space]\n",
        "\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "tN6ltGFNOX3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select rouge1, rouge2, rougeL\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Function to calculate rouge\n",
        "def compute_rouge(human_text, gpt_text):\n",
        "    human_text = preprocess(human_text)\n",
        "    gpt_text = preprocess(gpt_text)\n",
        "    scores = scorer.score(human_text, gpt_text)\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "earvBz6oPE4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROUGE scores\n",
        "gpt_variants = ['cot', 'mp', 'ps', 'sc']\n",
        "\n",
        "for variant in gpt_variants:\n",
        "    df[f'rouge1_f1_{variant}'] = df.apply(\n",
        "        lambda row: compute_rouge(row['human_explanation'], row[f'{variant}_gpt_explanation'])['rouge1'].fmeasure,\n",
        "        axis=1\n",
        "    )\n",
        "    df[f'rouge2_f1_{variant}'] = df.apply(\n",
        "        lambda row: compute_rouge(row['human_explanation'], row[f'{variant}_gpt_explanation'])['rouge2'].fmeasure,\n",
        "        axis=1\n",
        "    )\n",
        "    df[f'rougeL_f1_{variant}'] = df.apply(\n",
        "        lambda row: compute_rouge(row['human_explanation'], row[f'{variant}_gpt_explanation'])['rougeL'].fmeasure,\n",
        "        axis=1\n",
        "    )\n"
      ],
      "metadata": {
        "id": "e2bNIPM9QYUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview df to verify output successful\n",
        "df.head()"
      ],
      "metadata": {
        "id": "APIbIdBZ6bCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROUGE mean scores\n",
        "rouge_means = df[\n",
        "    [\n",
        "        'rouge1_f1_cot', 'rouge2_f1_cot', 'rougeL_f1_cot',\n",
        "        'rouge1_f1_mp',  'rouge2_f1_mp',  'rougeL_f1_mp',\n",
        "        'rouge1_f1_ps',  'rouge2_f1_ps',  'rougeL_f1_ps',\n",
        "        'rouge1_f1_sc',  'rouge2_f1_sc',  'rougeL_f1_sc'\n",
        "    ]\n",
        "].mean()\n",
        "\n",
        "# Calculate ROUGE standard deviation\n",
        "rouge_stds = df[\n",
        "    [\n",
        "        'rouge1_f1_cot', 'rouge2_f1_cot', 'rougeL_f1_cot',\n",
        "        'rouge1_f1_mp',  'rouge2_f1_mp',  'rougeL_f1_mp',\n",
        "        'rouge1_f1_ps',  'rouge2_f1_ps',  'rougeL_f1_ps',\n",
        "        'rouge1_f1_sc',  'rouge2_f1_sc',  'rougeL_f1_sc'\n",
        "    ]\n",
        "].std()\n",
        "\n",
        "print(rouge_means)\n",
        "print(rouge_stds)\n"
      ],
      "metadata": {
        "id": "WOjAtcKJRAYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine low, moderate, and high range\n",
        "\n",
        "# Select ROUGE columns\n",
        "rouge_cols = [\n",
        "    'rouge1_f1_cot', 'rouge2_f1_cot', 'rougeL_f1_cot',\n",
        "    'rouge1_f1_mp',  'rouge2_f1_mp',  'rougeL_f1_mp',\n",
        "    'rouge1_f1_ps',  'rouge2_f1_ps',  'rougeL_f1_ps',\n",
        "    'rouge1_f1_sc',  'rouge2_f1_sc',  'rougeL_f1_sc'\n",
        "]\n",
        "\n",
        "rouge_df = df[rouge_cols]\n",
        "\n",
        "# Restructure for analysis\n",
        "rouge_long = rouge_df.melt(var_name='rouge_metric', value_name='score')\n",
        "\n",
        "# Plot distributions\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.kdeplot(data=rouge_long, x='score', hue='rouge_metric', fill=True, common_norm=False, alpha=0.5)\n",
        "plt.title('ROUGE Score Distributions')\n",
        "plt.xlabel('ROUGE Score')\n",
        "plt.ylabel('Density')\n",
        "plt.legend(loc='upper right', fontsize='small')\n",
        "plt.show()\n",
        "\n",
        "# Print quantiles\n",
        "quantiles = rouge_long.groupby('rouge_metric')['score'].quantile([0.25, 0.5, 0.75]).unstack()\n",
        "print(quantiles)\n",
        "\n"
      ],
      "metadata": {
        "id": "UtR1Tbc_Nu-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Visualizing Agreement and ROUGE scores together. This section prepares a\n",
        "styled df that display binary label agreement and ROUGE-L F1 scores\n",
        "for each 'case_id' and 'criteria'. The cells are colour coded to visually distinguish\n",
        "between low, moderate, and high scores/agreement levels for quicker inspection.\n",
        "'''\n",
        "# Determine binary agreement (1 if labels equal, else 0)\n",
        "for variant in ['cot', 'mp', 'ps', 'sc']:\n",
        "    df[f'agreement_{variant}'] = (df['human_label'] == df[f'{variant}_gpt_label']).astype(int)\n",
        "\n",
        "# Select columns\n",
        "columns_to_show = ['case_id', 'criteria']\n",
        "\n",
        "# Add agreement and rouge columns per variant in desired order\n",
        "for variant in ['cot', 'mp', 'ps', 'sc']:\n",
        "    columns_to_show.append(f'agreement_{variant}')\n",
        "    for metric in ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']:\n",
        "        columns_to_show.append(f'{metric}_{variant}')\n",
        "\n",
        "df_display = df[columns_to_show].copy()\n",
        "\n",
        "# Define color functions to present low, moderate, high range\n",
        "\n",
        "def color_agreement(val):\n",
        "    if val == 1:\n",
        "        return 'background-color: lightgreen'\n",
        "    else:\n",
        "        return 'background-color: salmon'\n",
        "\n",
        "def color_rouge(val):\n",
        "    if val < 0.05:\n",
        "        return 'background-color: orange'\n",
        "    elif val < 0.15:\n",
        "        return 'background-color: yellow'\n",
        "    else:\n",
        "        return 'background-color: lightgreen'\n",
        "\n",
        "# Apply styling\n",
        "agreement_cols = [col for col in df_display.columns if col.startswith('agreement')]\n",
        "rouge_cols = [col for col in df_display.columns if col.startswith('rouge')]\n",
        "\n",
        "styled_df = df_display.style.applymap(color_agreement, subset=agreement_cols).applymap(color_rouge, subset=rouge_cols)\n",
        "\n",
        "# Print\n",
        "styled_df\n"
      ],
      "metadata": {
        "id": "IhlagSlSaKeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This section identifies specific rows from the dataset that represent\n",
        "interesting cases for deeper qualitative analysis based on label agreement\n",
        "and ROUGE-L scores.\n",
        "'''\n",
        "rows_of_interest = []\n",
        "\n",
        "for variant in ['cot', 'mp', 'ps', 'sc']:\n",
        "    agree_col = f'agreement_{variant}'\n",
        "    rouge_col = f'rougeL_f1_{variant}'\n",
        "    human_exp_col = 'human_explanation'\n",
        "    gpt_exp_col = f'{variant}_gpt_explanation'\n",
        "    gpt_label_col = f'{variant}_gpt_label'\n",
        "\n",
        "    # CASE 1: Agreement with low ROUGE\n",
        "    mask1 = (df_display[agree_col] == 1) & (df_display[rouge_col] < 0.05)\n",
        "    case1 = df.loc[mask1, [\n",
        "        'case_id', 'criteria', 'human_label', gpt_label_col, human_exp_col, gpt_exp_col\n",
        "    ]].copy()\n",
        "    case1['variant'] = variant.upper()\n",
        "    case1['flag'] = 'AGREE / LOW ROUGE'\n",
        "\n",
        "    # CASE 2: Disagreement with high ROUGE\n",
        "    mask2 = (df_display[agree_col] == 0) & (df_display[rouge_col] > 0.15)\n",
        "    case2 = df.loc[mask2, [\n",
        "        'case_id', 'criteria', 'human_label', gpt_label_col, human_exp_col, gpt_exp_col\n",
        "    ]].copy()\n",
        "    case2['variant'] = variant.upper()\n",
        "    case2['flag'] = 'DISAGREE / HIGH ROUGE'\n",
        "\n",
        "    # Standardize column names\n",
        "    case1.rename(columns={\n",
        "        gpt_label_col: 'gpt_label',\n",
        "        human_exp_col: 'human_explanation',\n",
        "        gpt_exp_col: 'gpt_explanation'\n",
        "    }, inplace=True)\n",
        "\n",
        "    case2.rename(columns={\n",
        "        gpt_label_col: 'gpt_label',\n",
        "        human_exp_col: 'human_explanation',\n",
        "        gpt_exp_col: 'gpt_explanation'\n",
        "    }, inplace=True)\n",
        "\n",
        "    rows_of_interest.append(case1)\n",
        "    rows_of_interest.append(case2)\n",
        "\n",
        "# Combine case 1 and case 2 cases\n",
        "df_review = pd.concat(rows_of_interest, ignore_index=True)\n",
        "df_review.sort_values(['case_id', 'criteria', 'variant'], inplace=True)\n",
        "\n",
        "# Print results\n",
        "df_review"
      ],
      "metadata": {
        "id": "2mhhc7RujCUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count occurrences case 1 and case 2\n",
        "flag_counts = df_review['flag'].value_counts()\n",
        "\n",
        "# Print counts\n",
        "print(\"Case Counts:\")\n",
        "print(flag_counts)"
      ],
      "metadata": {
        "id": "_OCzSPZhekhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Threshold for low ROUGE\n",
        "rouge_threshold = 0.05\n",
        "\n",
        "disagree_low_rouge_cases = []\n",
        "\n",
        "for variant in ['cot', 'mp', 'ps', 'sc']:\n",
        "    agreement_col = f'agreement_{variant}'\n",
        "    rouge_col = f'rougeL_f1_{variant}'\n",
        "    human_exp_col = 'human_explanation'\n",
        "    gpt_exp_col = f'{variant}_gpt_explanation'\n",
        "    human_label_col = 'human_label'\n",
        "    gpt_label_col = f'{variant}_gpt_label'\n",
        "\n",
        "    # Filter on original df\n",
        "    mask = (df[agreement_col] == 0) & (df[rouge_col] < rouge_threshold)\n",
        "\n",
        "    filtered = df.loc[mask, [\n",
        "        'case_id', 'criteria',\n",
        "        human_label_col, gpt_label_col,\n",
        "        human_exp_col, gpt_exp_col\n",
        "    ]].copy()\n",
        "\n",
        "    filtered['variant'] = variant.upper()\n",
        "    filtered['flag'] = 'DISAGREE / LOW ROUGE'\n",
        "\n",
        "    filtered.rename(columns={\n",
        "        human_label_col: 'human_label',\n",
        "        gpt_label_col: 'gpt_label',\n",
        "        human_exp_col: 'human_explanation',\n",
        "        gpt_exp_col: 'gpt_explanation'\n",
        "    }, inplace=True)\n",
        "\n",
        "    disagree_low_rouge_cases.append(filtered)\n",
        "\n",
        "df_disagree_low_rouge = pd.concat(disagree_low_rouge_cases, ignore_index=True)\n",
        "df_disagree_low_rouge.sort_values(['case_id', 'criteria', 'variant'], inplace=True)\n",
        "\n",
        "# Print results\n",
        "df_disagree_low_rouge\n"
      ],
      "metadata": {
        "id": "VVEUmy3Vipg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTSCORE"
      ],
      "metadata": {
        "id": "KBqZnbg4jSBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This section assesses the semantic similarity of ChatGPT-generated explanations\n",
        "compared to human evaluations using a pre-trained Sentence BERT transformer model.\n",
        "This method captures contextual meaning beyond N-gram overlap, offering\n",
        "a deeper insight into explanation quality.\n",
        "'''\n",
        "\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "h1hGEZjXjUga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "'paraphrase-multilingual-MiniLM-L12-v2' is a good choice for efficiency and performance across language\n",
        "- Dutch in this case.\n",
        "'''\n",
        "# Load model\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')"
      ],
      "metadata": {
        "id": "u_0X4KbKkmZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Note: the 'preprocess' function defined earlier for ROUGE will also be used here.\n",
        "This ensures consistent text preparation for both similarity metrics.\n",
        "'''\n",
        "def preprocess(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Normalize percentages: replace \"10%\" -> \"10 procent\"\n",
        "    text = re.sub(r'(\\d+)%', r'\\1 procent', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Dwf1OQMy8DeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to all explanation columns\n",
        "for col in ['human_explanation', 'cot_gpt_explanation', 'mp_gpt_explanation', 'ps_gpt_explanation']:\n",
        "    df[col] = df[col].apply(preprocess)\n",
        "\n",
        "# Compute semantic similarity for each prompt variant\n",
        "variants = ['cot', 'mp', 'ps', 'sc']\n",
        "\n",
        "for variant in variants:\n",
        "    col_human = 'human_explanation'\n",
        "    col_gpt = f'{variant}_gpt_explanation'\n",
        "    sim_scores = []\n",
        "\n",
        "    print(f\"Computing semantic similarity for: {variant.upper()}\")\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        emb1 = model.encode(row[col_human], convert_to_tensor=True)\n",
        "        emb2 = model.encode(row[col_gpt], convert_to_tensor=True)\n",
        "        score = util.pytorch_cos_sim(emb1, emb2).item()\n",
        "        sim_scores.append(score)\n",
        "\n",
        "    df[f'semantic_similarity_{variant}'] = sim_scores\n",
        "\n",
        "# Preview results\n",
        "df[['case_id', 'criteria',\n",
        "    'semantic_similarity_cot',\n",
        "    'semantic_similarity_mp',\n",
        "    'semantic_similarity_ps',\n",
        "    'semantic_similarity_sc']].head()"
      ],
      "metadata": {
        "id": "prcq7lrOk6AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine low, moderate, and high range\n",
        "\n",
        "# Select relevant columns\n",
        "sim_df = df[['semantic_similarity_cot', 'semantic_similarity_mp', 'semantic_similarity_ps', 'semantic_similarity_sc']]\n",
        "\n",
        "# Restructure for analysis\n",
        "sim_long = sim_df.melt(var_name='variant', value_name='similarity_score')\n",
        "\n",
        "# Plot distributions\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(data=sim_long, x='similarity_score', hue='variant', fill=True, common_norm=False, alpha=0.5)\n",
        "plt.title('Semantic Similarity Score Distributions by Variant')\n",
        "plt.xlabel('Semantic Similarity Score')\n",
        "plt.ylabel('Density')\n",
        "plt.show()\n",
        "\n",
        "# Print quantiles\n",
        "quantiles = sim_long.groupby('variant')['similarity_score'].quantile([0.25, 0.5, 0.75]).unstack()\n",
        "print(quantiles)\n"
      ],
      "metadata": {
        "id": "FNuFoI_OL7z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Visualizing Agreement and cosine similarity scores together. This section prepares a\n",
        "styled df that display binary label agreement and similarity scores\n",
        "for each 'case_id' and 'criteria'. The cells are colour coded to visually distinguish\n",
        "between low, moderate, and high scores/agreement levels for quicker inspection.\n",
        "'''\n",
        "# Select columns to show\n",
        "columns_to_show = ['case_id', 'criteria']\n",
        "\n",
        "# Add agreement and semantic similarity columns for each variant\n",
        "for variant in ['cot', 'mp', 'ps', 'sc']:\n",
        "    columns_to_show.append(f'agreement_{variant}')\n",
        "    columns_to_show.append(f'semantic_similarity_{variant}')\n",
        "\n",
        "# Slice relevant columns\n",
        "df_semantic = df[columns_to_show].copy()\n",
        "\n",
        "# Define color functions for low, moderate, and high range\n",
        "\n",
        "def color_agreement(val):\n",
        "    if val == 1:\n",
        "        return 'background-color: lightgreen'\n",
        "    else:\n",
        "        return 'background-color: salmon'\n",
        "\n",
        "def color_similarity(val):\n",
        "    if val < 0.2:\n",
        "        return 'background-color: orange'\n",
        "    elif val < 0.65:\n",
        "        return 'background-color: yellow'\n",
        "    else:\n",
        "        return 'background-color: lightgreen'\n",
        "\n",
        "# Apply styling\n",
        "agreement_cols = [col for col in df_semantic.columns if col.startswith('agreement')]\n",
        "similarity_cols = [col for col in df_semantic.columns if col.startswith('semantic_similarity')]\n",
        "\n",
        "styled_semantic = df_semantic.style \\\n",
        "    .applymap(color_agreement, subset=agreement_cols) \\\n",
        "    .applymap(color_similarity, subset=similarity_cols)\n",
        "\n",
        "# Print results\n",
        "styled_semantic\n"
      ],
      "metadata": {
        "id": "3alTY2_5lhaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This next section again identifies specific rows that represent intersting cases for\n",
        "further qualitative analysis based on label agreement and semantic similarity scores.\n",
        "'''"
      ],
      "metadata": {
        "id": "fuaDWZl5NVeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Merge semantic similarity scores back into the main df for combined analysis.\n",
        "'''\n",
        "# Merge semantic similarity scores into main df by case_id and criteria\n",
        "df_merged = df.merge(\n",
        "    df_semantic[['case_id', 'criteria',\n",
        "                 'semantic_similarity_cot',\n",
        "                 'semantic_similarity_mp',\n",
        "                 'semantic_similarity_ps',\n",
        "                 'semantic_similarity_sc']],\n",
        "    on=['case_id', 'criteria'],\n",
        "    how='left'\n",
        ")\n"
      ],
      "metadata": {
        "id": "rk_hv0ITo0XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "case1_list = []\n",
        "case2_list = []\n",
        "\n",
        "# Define thresholds (based on quantile analysis)\n",
        "low_sim_threshold = 0.2\n",
        "high_sim_threshold = 0.65\n",
        "\n",
        "for variant in ['cot', 'mp', 'ps', 'sc']:\n",
        "    agree_col = f'agreement_{variant}'\n",
        "    sim_col = f'semantic_similarity_{variant}_x'\n",
        "    human_exp_col = 'human_explanation'\n",
        "    gpt_exp_col = f'{variant}_gpt_explanation'\n",
        "    gpt_label_col = f'{variant}_gpt_label'\n",
        "\n",
        "    # CASE 1: Agreement with low semantic similarity\n",
        "    mask1 = (df_merged[agree_col] == 1) & (df_merged[sim_col] < low_sim_threshold)\n",
        "    case1 = df_merged.loc[mask1, [\n",
        "        'case_id', 'criteria', 'human_label', gpt_label_col, human_exp_col, gpt_exp_col, sim_col\n",
        "    ]].copy()\n",
        "    case1['variant'] = variant.upper()\n",
        "    case1['flag'] = 'AGREE / LOW SEMANTIC SIMILARITY'\n",
        "\n",
        "    # Rename columns for clarity\n",
        "    case1.rename(columns={\n",
        "        gpt_label_col: 'gpt_label',\n",
        "        human_exp_col: 'human_explanation',\n",
        "        gpt_exp_col: 'gpt_explanation',\n",
        "        sim_col: 'semantic_similarity'\n",
        "    }, inplace=True)\n",
        "    case1_list.append(case1)\n",
        "\n",
        "    # CASE 2: Disagreement with high semantic similarity\n",
        "    mask2 = (df_merged[agree_col] == 0) & (df_merged[sim_col] > high_sim_threshold)\n",
        "    case2 = df_merged.loc[mask2, [\n",
        "        'case_id', 'criteria', 'human_label', gpt_label_col, human_exp_col, gpt_exp_col, sim_col\n",
        "    ]].copy()\n",
        "    case2['variant'] = variant.upper()\n",
        "    case2['flag'] = 'DISAGREE / HIGH SEMANTIC SIMILARITY'\n",
        "\n",
        "    # Rename columns for clarity\n",
        "    case2.rename(columns={\n",
        "        gpt_label_col: 'gpt_label',\n",
        "        human_exp_col: 'human_explanation',\n",
        "        gpt_exp_col: 'gpt_explanation',\n",
        "        sim_col: 'semantic_similarity'\n",
        "    }, inplace=True)\n",
        "    case2_list.append(case2)\n",
        "\n",
        "# Concatenate separately\n",
        "df_agree_low_sim = pd.concat(case1_list, ignore_index=True)\n",
        "df_agree_low_sim.sort_values(['case_id', 'criteria', 'variant'], inplace=True)\n",
        "\n",
        "df_disagree_high_sim = pd.concat(case2_list, ignore_index=True)\n",
        "df_disagree_high_sim.sort_values(['case_id', 'criteria', 'variant'], inplace=True)\n",
        "\n",
        "df_agree_low_sim\n",
        "df_disagree_high_sim\n"
      ],
      "metadata": {
        "id": "SRnyIe25DDA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set threshold for low semantic similarity\n",
        "similarity_threshold = 0.2\n",
        "\n",
        "rows_of_interest = []\n",
        "\n",
        "for variant in ['cot', 'mp', 'ps', 'sc']:\n",
        "    agree_col = f'agreement_{variant}'\n",
        "    sim_col = f'semantic_similarity_{variant}_x'\n",
        "    human_label_col = 'human_label'\n",
        "    gpt_label_col = f'{variant}_gpt_label'\n",
        "    human_exp_col = 'human_explanation'\n",
        "    gpt_exp_col = f'{variant}_gpt_explanation'\n",
        "\n",
        "    mask = (df_merged[agree_col] == 0) & (df_merged[sim_col] < similarity_threshold)\n",
        "\n",
        "    filtered = df_merged.loc[mask, [\n",
        "        'case_id', 'criteria', human_label_col, gpt_label_col, human_exp_col, gpt_exp_col, sim_col\n",
        "    ]].copy()\n",
        "\n",
        "    filtered['variant'] = variant.upper()\n",
        "    filtered['flag'] = 'DISAGREE / LOW SEMANTIC SIMILARITY'\n",
        "\n",
        "    filtered.rename(columns={\n",
        "        gpt_label_col: 'gpt_label',\n",
        "        human_exp_col: 'human_explanation',\n",
        "        gpt_exp_col: 'gpt_explanation',\n",
        "        sim_col: 'semantic_similarity'\n",
        "    }, inplace=True)\n",
        "\n",
        "    rows_of_interest.append(filtered)\n",
        "\n",
        "df_disagree_low_sim = pd.concat(rows_of_interest, ignore_index=True)\n",
        "df_disagree_low_sim.sort_values(['case_id', 'criteria', 'variant'], inplace=True)\n",
        "\n",
        "# Print results\n",
        "df_disagree_low_sim\n"
      ],
      "metadata": {
        "id": "iz8xIjUl-AV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mean scores per prompting strategy"
      ],
      "metadata": {
        "id": "_NS4bf2mkcIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This section provides an aggregated view of the mean performance metrics\n",
        "for each prompting strategy. This table offers quick overview and comparison of\n",
        "the overall effectiveness of each method.\n",
        "'''"
      ],
      "metadata": {
        "id": "Bkje9KqMN2No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompting strategies\n",
        "variants = ['cot', 'mp', 'ps', 'sc']\n",
        "\n",
        "metrics = {\n",
        "    'agreement': lambda v: f'agreement_{v}',\n",
        "    'rouge1_f1': lambda v: f'rouge1_f1_{v}',\n",
        "    'rouge2_f1': lambda v: f'rouge2_f1_{v}',\n",
        "    'rougeL_f1': lambda v: f'rougeL_f1_{v}',\n",
        "    'semantic_similarity': lambda v: f'semantic_similarity_{v}_x'\n",
        "}\n",
        "\n",
        "# Prepare a results dictionary\n",
        "results = {v.upper(): {} for v in variants}\n",
        "\n",
        "# Calculate mean values\n",
        "for variant in variants:\n",
        "    var_upper = variant.upper()\n",
        "    for metric_name, col_func in metrics.items():\n",
        "        col_name = col_func(variant)\n",
        "        if col_name in df_merged.columns:\n",
        "            mean_val = df_merged[col_name].mean()\n",
        "            results[var_upper][metric_name] = mean_val\n",
        "        else:\n",
        "            results[var_upper][metric_name] = None  # or np.nan\n",
        "\n",
        "# Convert to df for nicer display\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = 'Variant'\n",
        "\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "2J_4H0UzBkAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONSISTENCY CHECK"
      ],
      "metadata": {
        "id": "fsqQUeqDr_VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This section focuses on evaluating the internal consistency of the GPT model\n",
        "themselves by comparing outputs from two separate runs for the same prompt\n",
        "strategy.\n",
        "'''"
      ],
      "metadata": {
        "id": "bZzNj8hJOgHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Upload the dataset specifically for consistency check. This dataset is expected to contain\n",
        "columns like cot_run1, cot_run2 etc.\n",
        "'''\n",
        "# Upload data\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\n",
        "    'INSERT DATASET',\n",
        "    encoding='utf-8',\n",
        "    header=0,\n",
        "    sep=';',\n",
        "    quotechar='\"',            # handles commas inside quotes\n",
        ")\n",
        "\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "YLrg1lnQsCq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "avLpZOVhsTZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "No missing values indicate that the transformation into csv datset was succesful\n",
        "and no data was lost. Now we can proceed to analysis.\n",
        "First, we calculate the Cohen's Kappa score across all CoT, MP, and PS data.\n",
        "'''"
      ],
      "metadata": {
        "id": "rfmro858Pya2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overall kappa for cot:\n",
        "kappa_cot = cohen_kappa_score(df['cot1'], df['cot2'])\n",
        "\n",
        "# Overall kappa for mp:\n",
        "kappa_mp = cohen_kappa_score(df['mp1'], df['mp2'])\n",
        "\n",
        "# Overall kappa for ps:\n",
        "kappa_ps = cohen_kappa_score(df['ps1'], df['ps2'])\n",
        "\n",
        "print(kappa_cot)\n",
        "print(kappa_mp)\n",
        "print(kappa_ps)"
      ],
      "metadata": {
        "id": "ZGB-EV8Jsehj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Next, we calculate Cohen's Kappa for CoT, MP, and PS per case.\n",
        "'''"
      ],
      "metadata": {
        "id": "Uosy6Aw-PtxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per case (for cot):\n",
        "for case in df['case_id'].unique():\n",
        "    subset = df[df['case_id'] == case]\n",
        "    print(f\"Case {case} kappa cot: \", cohen_kappa_score(subset['cot1'], subset['cot2']))"
      ],
      "metadata": {
        "id": "SyFZB7GEtgVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per case (for mp):\n",
        "for case in df['case_id'].unique():\n",
        "    subset = df[df['case_id'] == case]\n",
        "    print(f\"Case {case} kappa cot: \", cohen_kappa_score(subset['mp1'], subset['mp2']))"
      ],
      "metadata": {
        "id": "DSxhcsA3ts1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per case (for ps):\n",
        "for case in df['case_id'].unique():\n",
        "    subset = df[df['case_id'] == case]\n",
        "    print(f\"Case {case} kappa cot: \", cohen_kappa_score(subset['ps1'], subset['ps2']))"
      ],
      "metadata": {
        "id": "b7NZ7llatzsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Lastly, we look at Cohen's Kappa score per criteria\n",
        "'''"
      ],
      "metadata": {
        "id": "0ZBj0XsfPoYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criteria_list = df['criteria'].unique()\n",
        "\n",
        "for crit in criteria_list:\n",
        "    subset = df[df['criteria'] == crit]\n",
        "\n",
        "    kappa_cot = cohen_kappa_score(subset['cot1'], subset['cot2'])\n",
        "    kappa_mp = cohen_kappa_score(subset['mp1'], subset['mp2'])\n",
        "    kappa_ps = cohen_kappa_score(subset['ps1'], subset['ps2'])\n",
        "\n",
        "    print(f\"Criteria: {crit}\")\n",
        "    print(f\"  Cohen's Kappa cot1 vs cot2: {kappa_cot:.3f}\")\n",
        "    print(f\"  Cohen's Kappa mp1 vs mp2:   {kappa_mp:.3f}\")\n",
        "    print(f\"  Cohen's Kappa ps1 vs ps2:   {kappa_ps:.3f}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "LwcmQq51ugEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Check the unique values for cot1, cot2, ps1 and ps2.\n",
        "The output for criteria 'measures_and_actions' is nan.\n",
        "This means that there is no variability in the classification labels.\n",
        "To check whether this is correct we look at the unique values.\n",
        "'''"
      ],
      "metadata": {
        "id": "lMFnYCGVPkCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crit = \"measures_and_actions\"\n",
        "subset = df[df['criteria'] == crit]\n",
        "\n",
        "print(\"cot1 unique values:\", subset['cot1'].unique())\n",
        "print(\"cot2 unique values:\", subset['cot2'].unique())\n",
        "print(\"ps1 unique values:\", subset['ps1'].unique())\n",
        "print(\"ps2 unique values:\", subset['ps2'].unique())\n"
      ],
      "metadata": {
        "id": "oZe2o5Omvjt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Indeed, only 'accept' has been put as input classification, meaning that\n",
        "there is perfect agreement. However, Cohen's kappa can not determine its\n",
        "meaningfulness.\n",
        "'''"
      ],
      "metadata": {
        "id": "oSRcbFBuPeT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of dictionaries or rows with criteria and kappa values\n",
        "kappa_results = []\n",
        "\n",
        "criteria_list = df['criteria'].unique()\n",
        "for crit in criteria_list:\n",
        "    subset = df[df['criteria'] == crit]\n",
        "    kappa_results.append({\n",
        "        'criteria': crit,\n",
        "        'type': 'cot',\n",
        "        'kappa': cohen_kappa_score(subset['cot1'], subset['cot2'])\n",
        "    })\n",
        "    kappa_results.append({\n",
        "        'criteria': crit,\n",
        "        'type': 'mp',\n",
        "        'kappa': cohen_kappa_score(subset['mp1'], subset['mp2'])\n",
        "    })\n",
        "    kappa_results.append({\n",
        "        'criteria': crit,\n",
        "        'type': 'ps',\n",
        "        'kappa': cohen_kappa_score(subset['ps1'], subset['ps2'])\n",
        "    })\n",
        "\n",
        "kappa_df = pd.DataFrame(kappa_results)\n",
        "\n",
        "# Replace NaN with 0 for visualization\n",
        "kappa_df['kappa'] = kappa_df['kappa'].fillna(0)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=kappa_df, x='criteria', y='kappa', hue='type')\n",
        "plt.axhline(0, color='grey', linewidth=0.8)\n",
        "plt.title(\"Cohen's Kappa Scores per Criteria and Classification Type\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(-0.2, 1.05)\n",
        "plt.ylabel(\"Cohen's Kappa\")\n",
        "plt.legend(title='Classification Type')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wr1NTDcvwjlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot data for heatmap\n",
        "heatmap_data = kappa_df.pivot(index='criteria', columns='type', values='kappa')\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', vmin=-0.2, vmax=1, center=0)\n",
        "plt.title(\"Heatmap of Cohen's Kappa Scores\")\n",
        "plt.ylabel(\"Criteria\")\n",
        "plt.xlabel(\"Classification Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cGRTeM9Q2jrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather kappa scores per case into a list\n",
        "\n",
        "cases = df['case_id'].unique()\n",
        "results = []\n",
        "\n",
        "for case in cases:\n",
        "    subset = df[df['case_id'] == case]\n",
        "    kappa_cot = cohen_kappa_score(subset['cot1'], subset['cot2'])\n",
        "    kappa_mp = cohen_kappa_score(subset['mp1'], subset['mp2'])\n",
        "    kappa_ps = cohen_kappa_score(subset['ps1'], subset['ps2'])\n",
        "\n",
        "    results.append({\n",
        "        'case_id': case,\n",
        "        'cot': kappa_cot,\n",
        "        'mp': kappa_mp,\n",
        "        'ps': kappa_ps\n",
        "    })\n",
        "\n",
        "# Create df from results\n",
        "kappa_case_df = pd.DataFrame(results)\n",
        "\n",
        "# Prepare df for heatmap\n",
        "heatmap_data = kappa_case_df.set_index('case_id')\n",
        "\n",
        "# Fill NaN with 0\n",
        "heatmap_data = heatmap_data.fillna(0)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', vmin=-0.2, vmax=1, center=0)\n",
        "plt.title(\"Heatmap of Cohen's Kappa per Case and Classification Type\")\n",
        "plt.ylabel(\"Case ID\")\n",
        "plt.xlabel(\"Classification Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0Fw0foYXysYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Next I would like to create tables that present which cases and criteria\n",
        "do not match for the three strategies. This way I can do some more\n",
        "qualitative analysis into the underlying reason for the divergences.\n",
        "'''"
      ],
      "metadata": {
        "id": "SZajgpQpPQq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mismatch df for each method\n",
        "cot_mismatches = df[df['cot1'] != df['cot2']][['case_id', 'criteria', 'cot1', 'cot2']].sort_values(by='criteria')\n",
        "mp_mismatches = df[df['mp1'] != df['mp2']][['case_id', 'criteria', 'mp1', 'mp2']].sort_values(by='criteria')\n",
        "ps_mismatches = df[df['ps1'] != df['ps2']][['case_id', 'criteria', 'ps1', 'ps2']].sort_values(by='criteria')\n"
      ],
      "metadata": {
        "id": "rsTKTXrO5K9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print\n",
        "cot_mismatches\n",
        "mp_mismatches\n",
        "ps_mismatches"
      ],
      "metadata": {
        "id": "7hd3K5lH5MpX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}